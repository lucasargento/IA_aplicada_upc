{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JStmH-NXjU7Q"
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZRjonRvJ1TX"
   },
   "source": [
    "Fashion MNIST is a dataset from online fashion retailer Zalando consisting of a training set of 60,000 examples and a test set of 10,000 examples from one of 10 classes:\n",
    "\n",
    "| Label | Description |\n",
    "|-------|-------------|\n",
    "| 0     | T-shirt/top |\n",
    "| 1     | Trouser     |\n",
    "| 2     | Pullover    |\n",
    "| 3     | Dress       |\n",
    "| 4     | Coat        |\n",
    "| 5     | Sandal      |\n",
    "| 6     | Shirt       |\n",
    "| 7     | Sneaker     |\n",
    "| 8     | Bag         |\n",
    "| 9\t    | Ankle boot  |\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel. This pixel-value is an integer between 0 and 255.\n",
    "\n",
    "The goal is to predict the clothing category from the pixel-values from an image.\n",
    "\n",
    "The dataset can be directly loaded using a function from the keras library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6ClaTrfkmfG"
   },
   "outputs": [],
   "source": [
    "(X_train,y_train), (X_test,y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSBsKVRSL58i"
   },
   "source": [
    "Let's check the size of the training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GfrbDq2j1K8"
   },
   "outputs": [],
   "source": [
    "print('X training shape:',X_train.shape)\n",
    "print('X test shape:',X_test.shape)\n",
    "print('y training shape:',y_train.shape)\n",
    "print('y test shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tbnc3PniMtFS"
   },
   "source": [
    "Each example is composed by 28 rows and 28 columns that contain the 784 pixel values for an image. For example, these are the two first rows for a single example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BDp4oIqjoqP"
   },
   "outputs": [],
   "source": [
    "X_train[1][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qu1oSJFFN_st"
   },
   "source": [
    "The y arrays are one-dimensional vectors with the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18JuiSGkkMvp"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nO5ILStUOM5Y"
   },
   "source": [
    "We can display the first images in the training data to get an idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4YbzwwHkQh1"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']\n",
    "plt.figure(figsize=[10,10])\n",
    "\n",
    "for i in range(5):\n",
    "  n = int('15' + str(i + 1))\n",
    "  plt.subplot(n)\n",
    "  plt.imshow(X_train[i], cmap='gray')\n",
    "  plt.title(\"Class : {}\\n {}\".format(y_train[i], class_names[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iV90sogMRFaU"
   },
   "source": [
    "We scale the data into the 0-1 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1nYigv0RERx"
   },
   "outputs": [],
   "source": [
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scnhrEVgO6To"
   },
   "source": [
    "The network cannot work directly with categorical values, so we must convert them to numerical generating one boolean column (0/1) for each class category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6YDrIOwmvA9"
   },
   "outputs": [],
   "source": [
    "y_train_enc = to_categorical(y_train)\n",
    "y_test_enc = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-BQVMITm4-r"
   },
   "outputs": [],
   "source": [
    "y_train_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHDAPQoGPe5e"
   },
   "source": [
    "Now, we split the training data into a training and a validation set using the `train_test_split` function from `sklearn` as usual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbM36hi7m13E"
   },
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val = train_test_split(X_train, y_train_enc, test_size=0.2, random_state=35, stratify = y_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQkqLozDRYh5"
   },
   "source": [
    "Let's check the shapes of the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7xchzNWRXzS"
   },
   "outputs": [],
   "source": [
    "print('X training shape:',x_train.shape)\n",
    "print('X validation shape:',x_val.shape)\n",
    "print('y training shape:',y_train.shape)\n",
    "print('y validation shape:',y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvAYroW0Yc4w"
   },
   "source": [
    "## 2-layer Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZkTz0u3YmXs"
   },
   "source": [
    "---\n",
    "\n",
    "**Build a keras model that represents a two-layer MLP, i.e., an input layer, a single hidden layer and an output layer.**\n",
    "\n",
    "* **The input layer is a Flatten layer to convert the input into a single vector:**\n",
    "\n",
    "> `Flatten(input_shape=(28,28,1)`\n",
    "\n",
    "* **Add a Dense layer with 128 units and ReLu activation function as the hidden layer.**\n",
    "\n",
    "* **Add a Dense layer with 10 units (one per class label) and softmax activation function as the output layer.**\n",
    "\n",
    "**Compile the model and display the summary.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mg7P_IXxSJmq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qK28sz40b1GY"
   },
   "source": [
    "---\n",
    "\n",
    "**Train (fit) the model using a batch size of 256, 15 epochs and using the validation sets obtained above (x_val,y_val) as validation data.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAP3p4FTUiii"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ge4k-OmwcaSg"
   },
   "source": [
    "---\n",
    "\n",
    "**Plot the loss and accuracy for the training and validation data**\n",
    "\n",
    "**You can use the following code. It assumes that the object returning by the fit is named `model_history`, you can change the name.**\n",
    "\n",
    "**Study the plots. Does the model overfit the data? Check if the behavior suggests that training should stop earlier or more epochs are needed. In that case, modify the number of epochs and train the model again.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHf5XccfU865"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,4])\n",
    "\n",
    "accuracy = model_history.history['accuracy']\n",
    "val_accuracy = model_history.history['val_accuracy']\n",
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss,label='Training loss');\n",
    "plt.plot(val_loss,label='Validation loss');\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(accuracy,label='Training acc')\n",
    "plt.plot(val_accuracy,label='Validation acc')\n",
    "plt.legend(loc='upper left');\n",
    "\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7ikQlpFgsk1"
   },
   "source": [
    "---\n",
    "\n",
    "**Evaluate your final model on the test data using the `evaluate` method. Check the loss and the accuracy.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XkULCp8ei2Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-H5amdchx-q"
   },
   "source": [
    "---\n",
    "\n",
    "**Use the ``classification_report`` function from ``scikit-learn`` to get the classification metrics on each class and see which classes are most frequently misclassified.**\n",
    "\n",
    "**You can use the following code, substituting the `model` variable for the one containing your fitted model.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i69Eb6nvhwVi"
   },
   "outputs": [],
   "source": [
    "predict_test=model.predict(X_test);\n",
    "y_pred=np.argmax(predict_test,axis=1)\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyobdYjvhBTq"
   },
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQaCfhiRhIC6"
   },
   "source": [
    "---\n",
    "\n",
    "**Create a CNN model with the following layers:**\n",
    "\n",
    "* **A convolutional layer with 32 filters, a 3x3 kernel size, a ReLu activation function and `padding='same'`**\n",
    "\n",
    "* **A max-pooling layer with a 2x2 pool size**\n",
    "\n",
    "* **A flatten layer**\n",
    "\n",
    "* **A dense layer with 128 units and a ReLu activation function**\n",
    "\n",
    "* **A dense layer with 10 units and a softmax activation function**\n",
    "\n",
    "**Since the convolutional layer is the first, you must specify the size of the input as a parameter with**\n",
    "\n",
    "> `input_shape=(28, 28, 1)`\n",
    "\n",
    "**Compile the model and display the summary.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfPGm-ufiF7U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swOmuSPnmSkV"
   },
   "source": [
    "---\n",
    "\n",
    "**Train (fit) the model using a batch size of 256, 25 epochs and using the validation sets obtained above (x_val,y_val) as validation data.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vbf0vMYvjYPG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0rByg24m2VB"
   },
   "source": [
    "---\n",
    "\n",
    "**Plot the loss and accuracy for the training and validation data.**\n",
    "\n",
    "**Study the plots. Does the model overfit the data?**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hRnZ1BpkTI-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWljFWRho_Sw"
   },
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anaPelH_nmqe"
   },
   "source": [
    "---\n",
    "\n",
    "**If you think that the training should have stopped earlier to avoid overfitting, you can apply early stopping adding the following to the fit call:**\n",
    "\n",
    "> `callbacks=EarlyStopping(monitor='val_loss',patience=3)`\n",
    "\n",
    "**Plot the loss and accuracy for the training and validation data for the new model.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18xGnglBoH32"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBn7IdXmo4Tx"
   },
   "source": [
    "---\n",
    "\n",
    "**Select your final model with or without early stopping and apply it on the test data. Check the loss and the accuracy.**\n",
    "\n",
    "**Print a classification report.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAWchLT0lUEq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yac2gg-ypRSN"
   },
   "source": [
    "### Adding dropout to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNTVXvvOqCMw"
   },
   "source": [
    "---\n",
    "\n",
    "**A way to avoid overfitting is to add dropout layers. Add one dropout layer with a 0.25 rate to the previous model. Place it after the max-pooling layer. Add another after the first dense layer**\n",
    "\n",
    "**Train (fit) the model using a batch size of 256, 25 epochs and using the validation sets obtained above (x_val,y_val) as validation data.**\n",
    "\n",
    "**Plot the loss and accuracy for the training and validation data for the new model.**\n",
    "\n",
    "**You can also check how the dropout works with and without early stopping.**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmQKV1z5pSli"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gUTLLy3074Q"
   },
   "source": [
    "---\n",
    "\n",
    "**Train (fit) the model using a batch size of 256, 25 epochs and using the validation sets obtained above (x_val,y_val) as validation data.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCFL5VQxp8CA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXFVCfyursR6"
   },
   "source": [
    "---\n",
    "\n",
    "**Select the model with or without early stopping (but with dropout) and evaluate it on test data**\n",
    "\n",
    "**Print the classification report**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBbX04n72kns"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KamjXxnLsAW1"
   },
   "source": [
    "---\n",
    "**Now create a model with the following layers:**\n",
    "\n",
    "* **A convolutional layer with 32 filters and a 3x3 kernel size, a ReLu activation function and `padding='same'` (input layer)**\n",
    "\n",
    "* **A max-pooling layer with a 2x2 pooling size**\n",
    "\n",
    "* **A convolutional layer with 64 filters and a 3x3 kernel size, a ReLu activation function and `padding='same'`**\n",
    "\n",
    "* **A max-pooling layer with a 2x2 pooling size**\n",
    "\n",
    "* **A dropout layer with a 0.3 rate**\n",
    "\n",
    "* **A convolutional layer with 128 filters and a 3x3 kernel size, a ReLu activation function and `padding='same'`**\n",
    "\n",
    "* **A convolutional layer with 128 filters and a 3x3 kernel size, a ReLu activation function and `padding='same'`**\n",
    "\n",
    "* **A max-pooling layer with a 2x2 pooling size**\n",
    "\n",
    "* **A dropout layer with a 0.4 rate**\n",
    "\n",
    "* **A flatten layer**\n",
    "\n",
    "* **A dense layer with 512 units**\n",
    "\n",
    "* **A dropout layer with a 0.25 rate**\n",
    "\n",
    "* **A dense layer with 10 units and a softmax activation function**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVcew65WrteO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMZwDV4YwMjN"
   },
   "source": [
    "**Compile the model**\n",
    "\n",
    "**Train (fit) the model using a batch size of 256, 25 epochs and using the validation sets obtained above (x_val,y_val) as validation data.**\n",
    "\n",
    "**Plot the loss and accuracy for the training and validation data for the new model.**\n",
    "\n",
    "**Evaluate the model on the test set**\n",
    "\n",
    "**To complete the assignment, you can try to add early stopping to the mode or modify other parameters (number of filters, activation functions, dropout rate) and even adding new layers**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq2170zksL3M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
